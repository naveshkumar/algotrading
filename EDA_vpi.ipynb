{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "#### Author : Navesh Kumar\n",
    "#### Date : 21 Oct 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the location of the files\n",
    "source_path = 'D:/vpi/intraday_orderbook_snapshots/intraday_orderbook_snapshots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using below Libraries for analysis and visualisation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a file to understand the structure of the file\n",
    "sample_file_path = f'{source_path}/snapshot_2022_3_20_7_40_0.parquet'\n",
    "sample = pd.read_parquet(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 607 entries, 0 to 606\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   DeliveryStart  607 non-null    datetime64[ns, UTC]\n",
      " 1   Product        607 non-null    object             \n",
      " 2   BQty           607 non-null    float64            \n",
      " 3   Bid            607 non-null    object             \n",
      " 4   Ask            607 non-null    object             \n",
      " 5   AQty           607 non-null    float64            \n",
      " 6   Level          607 non-null    int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(1), object(3)\n",
      "memory usage: 33.3+ KB\n",
      "None\n",
      "              DeliveryStart                Product  BQty     Bid    Ask  AQty  \\\n",
      "0 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   4.3   -7.98   1.86  24.2   \n",
      "1 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   8.8  -10.90   5.00  24.3   \n",
      "2 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   0.8  -18.00  11.90   0.1   \n",
      "3 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   4.5  -22.10  15.45   1.2   \n",
      "4 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   1.0  -29.06  21.64   1.0   \n",
      "\n",
      "   Level  \n",
      "0      1  \n",
      "1      2  \n",
      "2      3  \n",
      "3      4  \n",
      "4      5  \n"
     ]
    }
   ],
   "source": [
    "print(sample.info())\n",
    "print(sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure and Domain understanding\n",
    "The file is a snapshot at a timestamp as noted by the file name </n>\n",
    "</n>\n",
    "Columns\n",
    "</n>\n",
    "1. DeliveryStart: Identifies the delivery time of the product </n>\n",
    "2. Product : given Xbid market in the EU; these are the HH, QH and H blocks identifiers</n>\n",
    "3. BQty : Bid volume</n>\n",
    "4. Bid</n>\n",
    "5. AQty: Offer Volume</n>\n",
    "6. Ask: Offer</n>\n",
    "7. Level : repeating 1 to 5: this represents the heirarchy of a data point in the market depth</n>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 1 : Descriptive analysis of data\n",
    "The given data set contains market depth Only till 5 levels of Market Depth per instrument (DeliveryStart) per Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 2 : Data quality and naming convention\n",
    "### 2.1 Data Quality\n",
    "The Products mentioned in this sample file indicate that the products being traded are\n",
    "1. XBID Hourly, Half Hourly, Quater Hourly products delivering in thier respective times\n",
    "2. Intraday Hourly, Half Hourly, Quater Hourly products delivering in thier respective times\n",
    "\n",
    "<i>While</i> the file names indicate in its format of YEAR_MONTH_DAY_HOUR_MINUTE_SECOND that the snapshot of trades should belong to the year 2022, however the data within the files are from 2023 in the YYYY signature in DeliveryStart column</br>\n",
    "\n",
    "Unless this data is specifically from futures market that may have a provision of trading such instruments (to best of my knowledge it does not exist) there is a <b><i>MISMATCH</i></b> in the file's name YYYY and the data it has captured in DeliveryStart. What makes my argument stronger is that the instruments have sufficient depth (5 levels of it) hence proving the mismatch as the products mentioned above are <b><i>SHORT TERM INSTRUMENT</i></b> that should be traded only near the <b><i>GATE CLOSURE</i></b>\n",
    "\n",
    "### 2.2 Time Zone\n",
    "I am doing my analysis w.r.t UTC. so for me Half Hour 1 will be 0 hour +00:30 while in the EU, where this exchange is presumably, it is +01:00. The file naming convention is based in UTC +01:00. Hence if I am analysing Half Hour 20; I am working with assumption of Delivery at 09:30:00 +00:00. the DeliveryStart column is based on this logic and follows the Gate Closure file-wise accordingly. However, the file naming convetion is in UTC, so the file that contains the last trade before 9.30 AM is in a file named _8_30. I will handle this TZ shift in the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for Analysing the data\n",
    "Since the data is available in timestamps, the continuity of the ticker is lost. We also have to account for the YYYY mismatch. Hence any analysis outside fundamental analsys should consider this discrepancy. My scope for this EDA is just fundamentals.</br>\n",
    "To understand the nature of the trading and really gain insight the data needs to be made available at a product level, for a unique product\n",
    "##### Important to note that this analysis focuses only on fundamentals\n",
    "Hence it is important that the observation period should be restricted to the most liquid period of a trade which is near its gate closure </n>\n",
    "\n",
    "<b>The Plan</b>\n",
    "1. specify the dates to observe: Origin and End </n>\n",
    "2. specify the Product Observation Start and Product Observation Start w.r.t. Gate Closure </n>\n",
    "3. Get all the Snapshot files within the origin and end dates merged as one dataframe </n>\n",
    "4. Filter product </n>\n",
    "\n",
    "This will get us an answer to the question : <i> During Origin to End dates, what was the fundamental Behaviour of Product XYZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Naming Convention\n",
    "Products deliver at a fixed time interval. For Half hourly products, there are 48 deliveries throughout the day.\n",
    "\n",
    "Since we want to keep the Date of Delivery and the delivery time itself flexible to observe the same delivery periods throughout the year we declare the instrument of the the product through its delivery date by simply indicating which one, chronologically, we are talking about. </n>\n",
    "\n",
    "Example: Half Hour product that delivers at 0100 can simply be called 3 as its the 3rd half hour product delivering </br></n>\n",
    "\n",
    "### YYYY file to data correction\n",
    "While as a user I would want to analyse the data from a fundamental perspective, the error correction for file naming convention both in the EDA and in the Algo Framework will be handled in the background. This makes it easier for me to think of trading strategies relevant to 2023 (albiet simple) as the 2022 and 2023 markets are contrasting.\n",
    "### File name TZ correction\n",
    "The file name local storage search will offset time by one hour to account for this issue.\n",
    "### Clock change\n",
    "Files dont seem to have clock change issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to analyse March 20th and 21st\n",
    "# collecting the Period of the Year we want to analyse\n",
    "Origin = [2023,5,2,0,0,0]\n",
    "End =    [2023,5,3,23,0,0]\n",
    "# On March 20th and 21st I am intrested in analysing the market behavour of Half Hour Instruments delivering 10 and 12 in the daytime\n",
    "#declaring the products as a dict - same feature used in algo framework\n",
    "product_dict = {\n",
    "    'QH':[40,41,42,43,44]\n",
    "}\n",
    "# Declaring a Period of Liquidity w.r.t gate closure\n",
    "# these are applicable to all selected products\n",
    "POS = 120 # Product Observation Start time 6 hours before gate closure #parent class ln 78\n",
    "POE = 0 # Product Observation End time 5 mins before gate closure #parent class ln 85\n",
    "# these are now needed to narrow down the files that contains our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# We have a description of the instrument and the product\n",
    "# Now it is converted into DeliveryStart for data loading\n",
    "\n",
    "# the same half hour will occur on every day between Origin and End\n",
    "# hence we need to calculate all the delivery start dates for given products\n",
    "# this is the first step towards constructing the product wise data\n",
    "# This is top down approach for keeping the code flexible\n",
    "# first we declare what we want as data\n",
    "# then we construct the date ranges we need for each data\n",
    "# then we select the appropriate file\n",
    "def product_to_time_mapping(product_catogory,product_delivery_time_string):\n",
    "    if product_catogory == 'HH':\n",
    "        hour, minute = map(int, product_delivery_time_string.split('_')[3:5])\n",
    "        product_number = (hour * 2) + (1 if minute == 0 else 2)\n",
    "        return f\"{product_catogory}{product_number:02}\" , product_number\n",
    "    \n",
    "    if product_catogory == 'QH':\n",
    "        hour, minute = map(int, product_delivery_time_string.split('_')[3:5])\n",
    "        product_number = (hour * 4) + (minute // 15) + 1\n",
    "        return f\"{product_catogory}{product_number:02}\", product_number\n",
    "\n",
    "\n",
    "def products_to_be_traded_generator(products,product_catogory,origin,end):\n",
    "    if product_catogory == 'HH':\n",
    "        collect_products_to_be_traded = []\n",
    "        delivery_period = origin\n",
    "        # rounding to nearest delivery period\n",
    "        m = delivery_period.minute\n",
    "        if m == 0:\n",
    "            delivery_period.replace(second = 0,microsecond = 0)\n",
    "        elif m <= 30:\n",
    "            delivery_period.replace(minute = 30, second = 0,microsecond = 0)\n",
    "        else:\n",
    "            (delivery_period + timedelta(hours=1)).replace(minute = 0, second = 0,microsecond = 0)\n",
    "\n",
    "        while delivery_period <= end:\n",
    "            collect_products_to_be_traded.append(delivery_period)\n",
    "            delivery_period = delivery_period + timedelta(minutes = 30)\n",
    "\n",
    "    if product_catogory == 'QH':\n",
    "        collect_products_to_be_traded = []\n",
    "        delivery_period = origin\n",
    "        m = delivery_period.minute\n",
    "        if m % 15 == 0:\n",
    "            delivery_period = delivery_period.replace(second=0, microsecond=0)\n",
    "        else:\n",
    "            # Calculate the nearest quarter-hour\n",
    "            minute_adjustment = (15 - m % 15) % 15\n",
    "            delivery_period = delivery_period + timedelta(minutes=minute_adjustment)\n",
    "            delivery_period = delivery_period.replace(second=0, microsecond=0)\n",
    "\n",
    "        # Collect quarter-hourly products\n",
    "        while delivery_period <= end:\n",
    "            collect_products_to_be_traded.append(delivery_period)\n",
    "            delivery_period = delivery_period + timedelta(minutes=15)\n",
    "\n",
    "    #convert the datetime into string for getting data\n",
    "    str_products_to_be_traded = []\n",
    "    for product in collect_products_to_be_traded:\n",
    "        str_products_to_be_traded.append(f\"{product.year}_{str(product.month)}_{str(product.day)}_{str(product.hour)}_{str(product.minute)}_{str(product.second)}\")\n",
    "\n",
    "    products_to_be_traded = []\n",
    "    #find the product number for this delivery period\n",
    "    for product_string in str_products_to_be_traded:\n",
    "        product_map_id , check_entry_prd = product_to_time_mapping(product_catogory,product_string)\n",
    "        #origin to end will contain all the delivery periods \n",
    "        # filtering the ones delcared in product\n",
    "        if check_entry_prd in products:\n",
    "            products_to_be_traded.append(f\"{product_map_id}-{product_string}\")\n",
    "    \n",
    "    return products_to_be_traded\n",
    "\n",
    "origin = datetime(*Origin) #parent class ln 44\n",
    "end = datetime(*End) #parent class ln 50\n",
    "\n",
    "# which can then be broken into \n",
    "product_category =[*product_dict.keys()][0]  #parent class ln 60\n",
    "_key_var = [*product_dict.keys()][0]\n",
    "products = product_dict[_key_var] #parent class ln 62\n",
    "\n",
    "#parent class ln 68\n",
    "products_to_be_traded = products_to_be_traded_generator(products,product_category,origin,end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the full Schedule of the Products we need </br>\n",
    "Using an element of the list we can get the data that we want to observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a product for analysis\n",
    "current_product = products_to_be_traded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "def GetData(source_path,current_product,POS,POE,product_category):\n",
    "    _ , deliveryStart = current_product.split('-')\n",
    "\n",
    "    #deliveryStart with POS and POE will locate the relevant files\n",
    "    # prd_id will filter the relevant data from the files\n",
    "    deliveryStart_dt = datetime(*[*map(lambda _d:int(_d),[*deliveryStart.split('_')])])\n",
    "\n",
    "    # IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "    deliveryStart_dt = deliveryStart_dt - timedelta(days=365) # 2022 and 2023 are not leap years\n",
    "\n",
    "    # IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "    deliveryStart_dt = deliveryStart_dt - timedelta(minutes=60) # TZ -01:00\n",
    "\n",
    "    # Gate Closure Calculations is kept at zero as the snapshots are at 5 min interval and so is the GC\n",
    "    deliveryStart_dt_GC = deliveryStart_dt + timedelta(minutes=0)\n",
    "\n",
    "    #calculating the POS and POE\n",
    "    POS_dt = deliveryStart_dt_GC - timedelta(minutes=POS)\n",
    "    POE_dt = deliveryStart_dt_GC - timedelta(minutes=POE)\n",
    "\n",
    "    # name of the files we need to collect\n",
    "    collect_files = []\n",
    "    collect_this = POS_dt\n",
    "    while collect_this <= POE_dt:\n",
    "        collect_this_str = f\"snapshot_{collect_this.year}_{collect_this.month}_{collect_this.day}_{collect_this.hour}_{collect_this.minute}_{collect_this.second}.parquet\"\n",
    "        collect_files.append(collect_this_str)\n",
    "        collect_this = collect_this + timedelta(minutes=5)\n",
    "\n",
    "    # get the files from the data location    \n",
    "    matching_files_dataframes = []\n",
    "    for file in collect_files:\n",
    "        file_path = os.path.join(source_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        file_str = os.path.splitext(file)[0].replace('snapshot_', '')\n",
    "        file_str = datetime(*[*map(lambda _d:int(_d) ,file_str.split('_')) ])\n",
    "        df['timestamp'] = file_str\n",
    "        matching_files_dataframes.append(df)\n",
    "\n",
    "    if matching_files_dataframes:\n",
    "        combined_df = pd.concat(matching_files_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    all_products = {\n",
    "                    'HH': 'XBID_Half_Hour',\n",
    "                    'QH': 'XBID_Quarter_Hour',\n",
    "                    'H' : 'XBID_Hour'\n",
    "                    }\n",
    "    filter_product = all_products[product_category]\n",
    "    combined_df = combined_df[combined_df['Product'] == filter_product]\n",
    "\n",
    "    # filtering the instrument\n",
    "    # formatting filter_instrument for filter query\n",
    "    \n",
    "    filter_instrument = deliveryStart_dt\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to UTC +00:00\n",
    "    filter_instrument = filter_instrument + timedelta(minutes=60)\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to 2023\n",
    "    filter_instrument = filter_instrument + timedelta(days=365)\n",
    "    filter_instrument = filter_instrument.replace(tzinfo=timezone.utc)\n",
    "    filter_instrument = pd.Timestamp(filter_instrument)\n",
    "\n",
    "    combined_df = combined_df[combined_df['DeliveryStart']==filter_instrument]\n",
    "\n",
    "    # Important!! : Correcting the timestamp bringing it back to 2023\n",
    "    combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(days=365)\n",
    "    # Important!! : Correcting the timestamp for time zone\n",
    "    combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(hours=1)\n",
    "    \n",
    "    combined_df = combined_df.reset_index()\n",
    "\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to 2023\n",
    "    GC = deliveryStart_dt_GC + timedelta(days=365)\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to UTC +00:00\n",
    "    GC = GC + timedelta(minutes=60)\n",
    "\n",
    "\n",
    "    return combined_df , GC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_data , GC = GetData(source_path,current_product,POS,POE,product_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>DeliveryStart</th>\n",
       "      <th>Product</th>\n",
       "      <th>BQty</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>AQty</th>\n",
       "      <th>Level</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>14.6</td>\n",
       "      <td>63.61</td>\n",
       "      <td>69.67</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-02 07:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>26.9</td>\n",
       "      <td>59.05</td>\n",
       "      <td>72.94</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-02 07:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>1.5</td>\n",
       "      <td>54.40</td>\n",
       "      <td>79.73</td>\n",
       "      <td>11.9</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-02 07:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.58</td>\n",
       "      <td>84.49</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-02 07:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>3.7</td>\n",
       "      <td>38.93</td>\n",
       "      <td>88.08</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-05-02 07:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>8381</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>12.1</td>\n",
       "      <td>76.38</td>\n",
       "      <td>82.83</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-02 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>8382</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>3.2</td>\n",
       "      <td>72.76</td>\n",
       "      <td>88.87</td>\n",
       "      <td>13.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-05-02 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>8383</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>1.3</td>\n",
       "      <td>66.70</td>\n",
       "      <td>94.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-05-02 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>8384</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>2.0</td>\n",
       "      <td>63.12</td>\n",
       "      <td>100.96</td>\n",
       "      <td>10.8</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-05-02 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>8385</td>\n",
       "      <td>2023-05-02 09:45:00+00:00</td>\n",
       "      <td>XBID_Quarter_Hour</td>\n",
       "      <td>4.1</td>\n",
       "      <td>56.53</td>\n",
       "      <td>110.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-05-02 09:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index             DeliveryStart            Product  BQty    Bid     Ask  \\\n",
       "0       71 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour  14.6  63.61   69.67   \n",
       "1       72 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour  26.9  59.05   72.94   \n",
       "2       73 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   1.5  54.40   79.73   \n",
       "3       74 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   3.0  49.58   84.49   \n",
       "4       75 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   3.7  38.93   88.08   \n",
       "..     ...                       ...                ...   ...    ...     ...   \n",
       "120   8381 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour  12.1  76.38   82.83   \n",
       "121   8382 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   3.2  72.76   88.87   \n",
       "122   8383 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   1.3  66.70   94.00   \n",
       "123   8384 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   2.0  63.12  100.96   \n",
       "124   8385 2023-05-02 09:45:00+00:00  XBID_Quarter_Hour   4.1  56.53  110.00   \n",
       "\n",
       "     AQty  Level           timestamp  \n",
       "0    17.5      1 2023-05-02 07:45:00  \n",
       "1     7.6      2 2023-05-02 07:45:00  \n",
       "2    11.9      3 2023-05-02 07:45:00  \n",
       "3     2.7      4 2023-05-02 07:45:00  \n",
       "4     9.9      5 2023-05-02 07:45:00  \n",
       "..    ...    ...                 ...  \n",
       "120   2.1      1 2023-05-02 09:45:00  \n",
       "121  13.1      2 2023-05-02 09:45:00  \n",
       "122   0.5      3 2023-05-02 09:45:00  \n",
       "123  10.8      4 2023-05-02 09:45:00  \n",
       "124   5.0      5 2023-05-02 09:45:00  \n",
       "\n",
       "[125 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 5, 2, 9, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 1\n",
    "XBID markets have more liquidity than Intraday Local markets </br>\n",
    "For this analysis we will focus only on XBID markets </br>\n",
    "<i>We will also develop the algoframework for just XBID</i> however the framework is flexible enough to be extended for local markets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 725 entries, 0 to 724\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   index          725 non-null    int64              \n",
      " 1   DeliveryStart  725 non-null    datetime64[ns, UTC]\n",
      " 2   Product        725 non-null    object             \n",
      " 3   BQty           725 non-null    float64            \n",
      " 4   Bid            725 non-null    object             \n",
      " 5   Ask            725 non-null    object             \n",
      " 6   AQty           725 non-null    float64            \n",
      " 7   Level          725 non-null    int64              \n",
      " 8   timestamp      725 non-null    datetime64[ns]     \n",
      "dtypes: datetime64[ns, UTC](1), datetime64[ns](1), float64(2), int64(2), object(3)\n",
      "memory usage: 51.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Visualising the bid offer spread till date closure\n",
    "foundation_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_data['Bid'] = pd.to_numeric(foundation_data['Bid'],errors='coerce')\n",
    "foundation_data['Ask'] = pd.to_numeric(foundation_data['Ask'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named 'foundation' and it has 'Bid', 'Ask', 'Level', and 'timestamp' columns\n",
    "# Convert 'timestamp' to datetime if it's not already\n",
    "foundation_data['timestamp'] = pd.to_datetime(foundation_data['timestamp'])\n",
    "\n",
    "# Create a long-form DataFrame with 'Bid' and 'Ask' values combined\n",
    "df_long = pd.melt(foundation_data, id_vars=['timestamp', 'Level'], value_vars=['Bid', 'Ask'], \n",
    "                  var_name='PriceType', value_name='Price')\n",
    "\n",
    "# Create the box plot where all 5 levels are combined in a single box per timestamp\n",
    "fig = px.box(df_long, \n",
    "             x='timestamp', \n",
    "             y='Price', \n",
    "             color='PriceType',  # Separate Bid and Ask into different colors\n",
    "             title=\"Bid and Ask Box Plot for Each Timestamp\",\n",
    "             labels={'Price': 'Price (Bid/Ask)', 'timestamp': 'Time'})\n",
    "\n",
    "# Update the layout for better visibility\n",
    "fig.update_layout(\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Price (Bid/Ask)',\n",
    "    boxmode='group'  # Group Bid and Ask by timestamp\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show(renderer = 'browser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_foundation = []\n",
    "for _d in range(0,5):\n",
    "    current_product = products_to_be_traded[_d]\n",
    "    foundation_data , _ = GetData(source_path,current_product,POS,POE,product_category)\n",
    "    foundation_data['Bid'] = pd.to_numeric(foundation_data['Bid'],errors='coerce')\n",
    "    foundation_data['Ask'] = pd.to_numeric(foundation_data['Ask'],errors='coerce')\n",
    "    \n",
    "    combined_foundation.append(foundation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_foundation is a list of 5 DataFrames similar to 'foundation'\n",
    "# Each DataFrame has 'timestamp', 'Bid', 'Ask', 'Level' columns\n",
    "num_dataframes = len(combined_foundation)\n",
    "\n",
    "# Create subplots with one row for each DataFrame\n",
    "fig = make_subplots(rows=num_dataframes, cols=1, shared_xaxes=True, \n",
    "                    subplot_titles=[f\"DataFrame {i+1}\" for i in range(num_dataframes)],\n",
    "                    vertical_spacing=0.05)\n",
    "\n",
    "# Loop through each DataFrame in the list\n",
    "for i, df in enumerate(combined_foundation):\n",
    "    # Convert 'timestamp' to datetime if it's not already\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Create a long-form DataFrame with 'Bid' and 'Ask' values combined\n",
    "    df_long = pd.melt(df, id_vars=['timestamp', 'Level'], value_vars=['Bid', 'Ask'], \n",
    "                      var_name='PriceType', value_name='Price')\n",
    "\n",
    "    # Create a box plot for each DataFrame and add it to the subplot for that DataFrame\n",
    "    for price_type in ['Bid', 'Ask']:\n",
    "        df_filtered = df_long[df_long['PriceType'] == price_type]\n",
    "\n",
    "        # Add the box plot for this DataFrame\n",
    "        fig.add_trace(go.Box(\n",
    "            x=df_filtered['timestamp'], \n",
    "            y=df_filtered['Price'],\n",
    "            name=price_type,\n",
    "            boxmean='sd',  # Optional: Display mean and standard deviation\n",
    "            marker_color='blue' if price_type == 'Bid' else 'orange',\n",
    "            showlegend=(i == 0)),  # Show legend only for the first subplot\n",
    "            row=i+1, col=1  # Positioning the plot in the subplot\n",
    "        )\n",
    "\n",
    "# Update the layout for better visibility\n",
    "fig.update_layout(\n",
    "    height=600 * num_dataframes,  # Adjust height based on number of subplots\n",
    "    title_text=\"Bid and Ask Box Plots Across Multiple DataFrames\",\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Price (Bid/Ask)',\n",
    "    boxmode='group',  # Group Bid and Ask together\n",
    "    showlegend=True  # Ensure legend is shown\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show(renderer = 'browser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Proof \n",
    "#### That the Momentum is 'fair risk' given the prices remain within same variance\n",
    "Using ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA test result: F_onewayResult(statistic=np.float64(1264.930377643819), pvalue=np.float64(6.726003178181657e-192))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming combined_foundation is a list of 5 DataFrames each containing 'Bid' and 'Ask'\n",
    "\n",
    "# Combine all the data into one DataFrame to make it easier for ANOVA\n",
    "all_data = []\n",
    "\n",
    "for i, df in enumerate(combined_foundation):\n",
    "    # Add a 'Dataset' column to identify the DataFrame\n",
    "    df['Dataset'] = f'Dataset_{i+1}'\n",
    "    \n",
    "    # Keep only the 'Bid' and 'Ask' columns, and melt them into a long format\n",
    "    df_long = pd.melt(df[['Bid', 'Ask', 'Dataset']], id_vars=['Dataset'], var_name='PriceType', value_name='Price')\n",
    "    \n",
    "    # Append this data to the combined list\n",
    "    all_data.append(df_long)\n",
    "\n",
    "# Combine the list into a single DataFrame\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Perform ANOVA: Group by 'PriceType' and 'Dataset' to compare across groups\n",
    "anova_result = stats.f_oneway(\n",
    "    combined_df[combined_df['PriceType'] == 'Bid']['Price'],\n",
    "    combined_df[combined_df['PriceType'] == 'Ask']['Price']\n",
    ")\n",
    "\n",
    "# Output the results of the ANOVA test\n",
    "print('ANOVA test result:', anova_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA result for Bids between Dataset_1 and Dataset_2: F_onewayResult(statistic=np.float64(699.1774126413815), pvalue=np.float64(4.0189028851656515e-74))\n",
      "ANOVA result for Asks between Dataset_1 and Dataset_2: F_onewayResult(statistic=np.float64(679.7489992776466), pvalue=np.float64(5.270028474442268e-73))\n",
      "ANOVA result for Bids between Dataset_1 and Dataset_3: F_onewayResult(statistic=np.float64(214.53985581378714), pvalue=np.float64(2.007353653249401e-35))\n",
      "ANOVA result for Asks between Dataset_1 and Dataset_3: F_onewayResult(statistic=np.float64(227.21220149015016), pvalue=np.float64(6.9274992220521275e-37))\n",
      "ANOVA result for Bids between Dataset_1 and Dataset_4: F_onewayResult(statistic=np.float64(94.82027677644471), pvalue=np.float64(3.488911298630108e-19))\n",
      "ANOVA result for Asks between Dataset_1 and Dataset_4: F_onewayResult(statistic=np.float64(108.64759125088992), pvalue=np.float64(2.4708276751846154e-21))\n",
      "ANOVA result for Bids between Dataset_1 and Dataset_5: F_onewayResult(statistic=np.float64(2.937917490156336), pvalue=np.float64(0.08777164857214997))\n",
      "ANOVA result for Asks between Dataset_1 and Dataset_5: F_onewayResult(statistic=np.float64(11.544360527089431), pvalue=np.float64(0.000791489763011439))\n",
      "ANOVA result for Bids between Dataset_2 and Dataset_3: F_onewayResult(statistic=np.float64(177.09730407333885), pvalue=np.float64(7.438198152441678e-31))\n",
      "ANOVA result for Asks between Dataset_2 and Dataset_3: F_onewayResult(statistic=np.float64(160.08349889452919), pvalue=np.float64(1.213262371707156e-28))\n",
      "ANOVA result for Bids between Dataset_2 and Dataset_4: F_onewayResult(statistic=np.float64(315.2608238047153), pvalue=np.float64(4.49743451415485e-46))\n",
      "ANOVA result for Asks between Dataset_2 and Dataset_4: F_onewayResult(statistic=np.float64(308.83120611937903), pvalue=np.float64(1.875770765431079e-45))\n",
      "ANOVA result for Bids between Dataset_2 and Dataset_5: F_onewayResult(statistic=np.float64(518.280042011736), pvalue=np.float64(1.087219501069837e-62))\n",
      "ANOVA result for Asks between Dataset_2 and Dataset_5: F_onewayResult(statistic=np.float64(519.543200683501), pvalue=np.float64(8.860318037085732e-63))\n",
      "ANOVA result for Bids between Dataset_3 and Dataset_4: F_onewayResult(statistic=np.float64(24.429872258205208), pvalue=np.float64(1.4198034027306165e-06))\n",
      "ANOVA result for Asks between Dataset_3 and Dataset_4: F_onewayResult(statistic=np.float64(26.752443795491747), pvalue=np.float64(4.769591049155836e-07))\n",
      "ANOVA result for Bids between Dataset_3 and Dataset_5: F_onewayResult(statistic=np.float64(133.2982818912644), pvalue=np.float64(5.81419123299302e-25))\n",
      "ANOVA result for Asks between Dataset_3 and Dataset_5: F_onewayResult(statistic=np.float64(132.3774549966582), pvalue=np.float64(7.86425818358718e-25))\n",
      "ANOVA result for Bids between Dataset_4 and Dataset_5: F_onewayResult(statistic=np.float64(50.42531535715128), pvalue=np.float64(1.3019484035152323e-11))\n",
      "ANOVA result for Asks between Dataset_4 and Dataset_5: F_onewayResult(statistic=np.float64(46.08821508535386), pvalue=np.float64(8.291034593152567e-11))\n"
     ]
    }
   ],
   "source": [
    "# Perform pairwise ANOVA for each pair of datasets\n",
    "for i in range(len(combined_foundation)):\n",
    "    for j in range(i + 1, len(combined_foundation)):\n",
    "        df_i = combined_foundation[i]\n",
    "        df_j = combined_foundation[j]\n",
    "        \n",
    "        # Perform ANOVA on the 'Bid' and 'Ask' columns between dataset i and dataset j\n",
    "        anova_result_bid = stats.f_oneway(df_i['Bid'], df_j['Bid'])\n",
    "        anova_result_ask = stats.f_oneway(df_i['Ask'], df_j['Ask'])\n",
    "        \n",
    "        print(f'ANOVA result for Bids between Dataset_{i+1} and Dataset_{j+1}:', anova_result_bid)\n",
    "        print(f'ANOVA result for Asks between Dataset_{i+1} and Dataset_{j+1}:', anova_result_ask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA test result for Bids: F_onewayResult(statistic=np.float64(234.4971288466361), pvalue=np.float64(1.6589043494662836e-122))\n",
      "ANOVA test result for Asks: F_onewayResult(statistic=np.float64(240.7849818262356), pvalue=np.float64(1.1702228509313808e-124))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming combined_foundation is a list of 5 DataFrames each containing 'Bid' and 'Ask'\n",
    "\n",
    "# Extract Bid and Ask values from each dataset\n",
    "bids = [df['Bid'].dropna() for df in combined_foundation]  # List of bid values from each DataFrame\n",
    "asks = [df['Ask'].dropna() for df in combined_foundation]  # List of ask values from each DataFrame\n",
    "\n",
    "# Perform ANOVA for Bids across the 5 datasets\n",
    "anova_result_bids = stats.f_oneway(*bids)\n",
    "\n",
    "# Perform ANOVA for Asks across the 5 datasets\n",
    "anova_result_asks = stats.f_oneway(*asks)\n",
    "\n",
    "# Output the results of the ANOVA test for Bids and Asks\n",
    "print('ANOVA test result for Bids:', anova_result_bids)\n",
    "print('ANOVA test result for Asks:', anova_result_asks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
