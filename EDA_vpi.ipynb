{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "#### Author : Navesh Kumar\n",
    "#### Date : 21 Oct 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the location of the files\n",
    "source_path = 'D:/vpi/intraday_orderbook_snapshots/intraday_orderbook_snapshots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using below Libraries for analysis and visualisation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a file to understand the structure of the file\n",
    "sample_file_path = f'{source_path}/snapshot_2022_3_20_7_40_0.parquet'\n",
    "sample = pd.read_parquet(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 607 entries, 0 to 606\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   DeliveryStart  607 non-null    datetime64[ns, UTC]\n",
      " 1   Product        607 non-null    object             \n",
      " 2   BQty           607 non-null    float64            \n",
      " 3   Bid            607 non-null    object             \n",
      " 4   Ask            607 non-null    object             \n",
      " 5   AQty           607 non-null    float64            \n",
      " 6   Level          607 non-null    int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(1), object(3)\n",
      "memory usage: 33.3+ KB\n",
      "None\n",
      "              DeliveryStart                Product  BQty     Bid    Ask  AQty  \\\n",
      "0 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   4.3   -7.98   1.86  24.2   \n",
      "1 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   8.8  -10.90   5.00  24.3   \n",
      "2 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   0.8  -18.00  11.90   0.1   \n",
      "3 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   4.5  -22.10  15.45   1.2   \n",
      "4 2023-01-01 00:15:00+00:00  Intraday_Quarter_Hour   1.0  -29.06  21.64   1.0   \n",
      "\n",
      "   Level  \n",
      "0      1  \n",
      "1      2  \n",
      "2      3  \n",
      "3      4  \n",
      "4      5  \n"
     ]
    }
   ],
   "source": [
    "print(sample.info())\n",
    "print(sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure and Domain understanding\n",
    "The file is a snapshot at a timestamp as noted by the file name </n>\n",
    "</n>\n",
    "Columns\n",
    "</n>\n",
    "1. DeliveryStart: Identifies the delivery time of the product </n>\n",
    "2. Product : given Xbid market in the EU; these are the HH, QH and H blocks identifiers</n>\n",
    "3. BQty : Bid volume</n>\n",
    "4. Bid</n>\n",
    "5. AQty: Offer Volume</n>\n",
    "6. Ask: Offer</n>\n",
    "7. Level : repeating 1 to 5: this represents the heirarchy of a data point in the market depth</n>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 1 : Descriptive analysis of data\n",
    "The given data set contains market depth Only till 5 levels of Market Depth per instrument (DeliveryStart) per Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 2 : Data quality and naming convention\n",
    "### 2.1 Data Quality\n",
    "The Products mentioned in this sample file indicate that the products being traded are\n",
    "1. XBID Hourly, Half Hourly, Quater Hourly products delivering in thier respective times\n",
    "2. Intraday Hourly, Half Hourly, Quater Hourly products delivering in thier respective times\n",
    "\n",
    "<i>While</i> the file names indicate in its format of YEAR_MONTH_DAY_HOUR_MINUTE_SECOND that the snapshot of trades should belong to the year 2022, however the data within the files are from 2023 in the YYYY signature in DeliveryStart column</br>\n",
    "\n",
    "Unless this data is specifically from futures market that may have a provision of trading such instruments (to best of my knowledge it does not exist) there is a <b><i>MISMATCH</i></b> in the file's name YYYY and the data it has captured in DeliveryStart. What makes my argument stronger is that the instruments have sufficient depth (5 levels of it) hence proving the mismatch as the products mentioned above are <b><i>SHORT TERM INSTRUMENT</i></b> that should be traded only near the <b><i>GATE CLOSURE</i></b>\n",
    "\n",
    "### 2.2 Time Zone\n",
    "I am doing my analysis w.r.t UTC. so for me Half Hour 1 will be 0 hour +00:30 while in the EU, where this exchange is presumably, it is +01:00. The file naming convention is based in UTC +01:00. Hence if I am analysing Half Hour 20; I am working with assumption of Delivery at 09:30:00 +00:00. the DeliveryStart column is based on this logic and follows the Gate Closure file-wise accordingly. However, the file naming convetion is in UTC, so the file that contains the last trade before 9.30 AM is in a file named _8_30. I will handle this TZ shift in the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for Analysing the data\n",
    "Since the data is available in timestamps, the continuity of the ticker is lost. We also have to account for the YYYY mismatch. Hence any analysis outside fundamental analsys should consider this discrepancy. My scope for this EDA is just fundamentals.</br>\n",
    "To understand the nature of the trading and really gain insight the data needs to be made available at a product level, for a unique product\n",
    "##### Important to note that this analysis focuses only on fundamentals\n",
    "Hence it is important that the observation period should be restricted to the most liquid period of a trade which is near its gate closure </n>\n",
    "\n",
    "<b>The Plan</b>\n",
    "1. specify the dates to observe: Origin and End </n>\n",
    "2. specify the Product Observation Start and Product Observation Start w.r.t. Gate Closure </n>\n",
    "3. Get all the Snapshot files within the origin and end dates merged as one dataframe </n>\n",
    "4. Filter product </n>\n",
    "\n",
    "This will get us an answer to the question : <i> During Origin to End dates, what was the fundamental Behaviour of Product XYZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Naming Convention\n",
    "Products deliver at a fixed time interval. For Half hourly products, there are 48 deliveries throughout the day.\n",
    "\n",
    "Since we want to keep the Date of Delivery and the delivery time itself flexible to observe the same delivery periods throughout the year we declare the instrument of the the product through its delivery date by simply indicating which one, chronologically, we are talking about. </n>\n",
    "\n",
    "Example: Half Hour product that delivers at 0100 can simply be called 3 as its the 3rd half hour product delivering </br></n>\n",
    "\n",
    "### YYYY file to data correction\n",
    "While as a user I would want to analyse the data from a fundamental perspective, the error correction for file naming convention both in the EDA and in the Algo Framework will be handled in the background. This makes it easier for me to think of trading strategies relevant to 2023 (albiet simple) as the 2022 and 2023 markets are contrasting.\n",
    "### File name TZ correction\n",
    "The file name local storage search will offset time by one hour to account for this issue.\n",
    "### Clock change\n",
    "Files dont seem to have clock change issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to analyse March 20th and 21st\n",
    "# collecting the Period of the Year we want to analyse\n",
    "Origin = [2023,5,2,0,0,0]\n",
    "End =    [2023,5,3,23,0,0]\n",
    "# On March 20th and 21st I am intrested in analysing the market behavour of Half Hour Instruments delivering 10 and 12 in the daytime\n",
    "#declaring the products as a dict - same feature used in algo framework\n",
    "product_dict = {\n",
    "    'QH':[40,48]\n",
    "}\n",
    "# Declaring a Period of Liquidity w.r.t gate closure\n",
    "# these are applicable to all selected products\n",
    "POS = 720 # Product Observation Start time 6 hours before gate closure\n",
    "POE = 0 # Product Observation End time 5 mins before gate closure\n",
    "# these are now needed to narrow down the files that contains our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which can then be broken into \n",
    "product_category =[*product_dict.keys()][0]\n",
    "_key_var = [*product_dict.keys()][0]\n",
    "products = product_dict[_key_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# We have a description of the instrument and the product\n",
    "# Now it is converted into DeliveryStart for data loading\n",
    "\n",
    "# the same half hour will occur on every day between Origin and End\n",
    "# hence we need to calculate all the delivery start dates for given products\n",
    "# this is the first step towards constructing the product wise data\n",
    "# This is top down approach for keeping the code flexible\n",
    "# first we declare what we want as data\n",
    "# then we construct the date ranges we need for each data\n",
    "# then we select the appropriate file\n",
    "def product_to_time_mapping(product_catogory,product_delivery_time_string):\n",
    "    if product_catogory == 'HH':\n",
    "        hour, minute = map(int, product_delivery_time_string.split('_')[3:5])\n",
    "        product_number = (hour * 2) + (1 if minute == 0 else 2)\n",
    "        return f\"{product_catogory}{product_number:02}\" , product_number\n",
    "    \n",
    "    if product_catogory == 'QH':\n",
    "        hour, minute = map(int, product_delivery_time_string.split('_')[3:5])\n",
    "        product_number = (hour * 4) + (minute // 15) + 1\n",
    "        return f\"{product_catogory}{product_number:02}\", product_number\n",
    "\n",
    "\n",
    "def products_to_be_traded_generator(products,product_catogory,origin,end):\n",
    "    if product_catogory == 'HH':\n",
    "        collect_products_to_be_traded = []\n",
    "        delivery_period = origin\n",
    "        # rounding to nearest delivery period\n",
    "        m = delivery_period.minute\n",
    "        if m == 0:\n",
    "            delivery_period.replace(second = 0,microsecond = 0)\n",
    "        elif m <= 30:\n",
    "            delivery_period.replace(minute = 30, second = 0,microsecond = 0)\n",
    "        else:\n",
    "            (delivery_period + timedelta(hours=1)).replace(minute = 0, second = 0,microsecond = 0)\n",
    "\n",
    "        while delivery_period <= end:\n",
    "            collect_products_to_be_traded.append(delivery_period)\n",
    "            delivery_period = delivery_period + timedelta(minutes = 30)\n",
    "\n",
    "    if product_catogory == 'QH':\n",
    "        collect_products_to_be_traded = []\n",
    "        delivery_period = origin\n",
    "        m = delivery_period.minute\n",
    "        if m % 15 == 0:\n",
    "            delivery_period = delivery_period.replace(second=0, microsecond=0)\n",
    "        else:\n",
    "            # Calculate the nearest quarter-hour\n",
    "            minute_adjustment = (15 - m % 15) % 15\n",
    "            delivery_period = delivery_period + timedelta(minutes=minute_adjustment)\n",
    "            delivery_period = delivery_period.replace(second=0, microsecond=0)\n",
    "\n",
    "        # Collect quarter-hourly products\n",
    "        while delivery_period <= end:\n",
    "            collect_products_to_be_traded.append(delivery_period)\n",
    "            delivery_period = delivery_period + timedelta(minutes=15)\n",
    "\n",
    "    #convert the datetime into string for getting data\n",
    "    str_products_to_be_traded = []\n",
    "    for product in collect_products_to_be_traded:\n",
    "        str_products_to_be_traded.append(f\"{product.year}_{str(product.month)}_{str(product.day)}_{str(product.hour)}_{str(product.minute)}_{str(product.second)}\")\n",
    "\n",
    "    products_to_be_traded = []\n",
    "    #find the product number for this delivery period\n",
    "    for product_string in str_products_to_be_traded:\n",
    "        product_map_id , check_entry_prd = product_to_time_mapping(product_catogory,product_string)\n",
    "        #origin to end will contain all the delivery periods \n",
    "        # filtering the ones delcared in product\n",
    "        if check_entry_prd in products:\n",
    "            products_to_be_traded.append(f\"{product_map_id}-{product_string}\")\n",
    "    \n",
    "    return products_to_be_traded\n",
    "\n",
    "\n",
    "products = products\n",
    "product_category = product_category\n",
    "origin = datetime(*Origin) \n",
    "end = datetime(*End) \n",
    "products_to_be_traded = products_to_be_traded_generator(products,product_category,origin,end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the full Schedule of the Products we need </br>\n",
    "Using an element of the list we can get the data that we want to observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a product for analysis\n",
    "current_product = products_to_be_traded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import timezone\n",
    "def GetData(current_product,POS,POE,product_category):\n",
    "    prd_id , deliveryStart = current_product.split('-')\n",
    "\n",
    "    #deliveryStart with POS and POE will locate the relevant files\n",
    "    # prd_id will filter the relevant data from the files\n",
    "    deliveryStart_dt = datetime(*[*map(lambda _d:int(_d),[*deliveryStart.split('_')])])\n",
    "\n",
    "    # IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "    deliveryStart_dt = deliveryStart_dt - timedelta(days=365) # 2022 and 2023 are not leap years\n",
    "\n",
    "    # IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "    deliveryStart_dt = deliveryStart_dt - timedelta(minutes=60) # TZ -01:00\n",
    "\n",
    "    # Gate Closure Calculations is kept at zero as the snapshots are at 5 min interval and so is the GC\n",
    "    deliveryStart_dt_GC = deliveryStart_dt + timedelta(minutes=0)\n",
    "\n",
    "    #calculating the POS and POE\n",
    "    POS_dt = deliveryStart_dt_GC - timedelta(minutes=POS)\n",
    "    POE_dt = deliveryStart_dt_GC - timedelta(minutes=POE)\n",
    "\n",
    "    # name of the files we need to collect\n",
    "    collect_files = []\n",
    "    collect_this = POS_dt\n",
    "    while collect_this <= POE_dt:\n",
    "        collect_this_str = f\"snapshot_{collect_this.year}_{collect_this.month}_{collect_this.day}_{collect_this.hour}_{collect_this.minute}_{collect_this.second}.parquet\"\n",
    "        collect_files.append(collect_this_str)\n",
    "        collect_this = collect_this + timedelta(minutes=5)\n",
    "\n",
    "    # get the files from the data location    \n",
    "    matching_files_dataframes = []\n",
    "    for file in collect_files:\n",
    "        file_path = os.path.join(source_path, file)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        file_str = os.path.splitext(file)[0].replace('snapshot_', '')\n",
    "        file_str = datetime(*[*map(lambda _d:int(_d) ,file_str.split('_')) ])\n",
    "        df['timestamp'] = file_str\n",
    "        matching_files_dataframes.append(df)\n",
    "\n",
    "    if matching_files_dataframes:\n",
    "        combined_df = pd.concat(matching_files_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    all_products = {\n",
    "                    'HH': 'XBID_Half_Hour',\n",
    "                    'QH': 'XBID_Quarter_Hour',\n",
    "                    'H' : 'XBID_Hour'\n",
    "                    }\n",
    "    filter_product = all_products[product_category]\n",
    "    combined_df = combined_df[combined_df['Product'] == filter_product]\n",
    "\n",
    "    # filtering the instrument\n",
    "    # formatting filter_instrument for filter query\n",
    "    \n",
    "    filter_instrument = deliveryStart_dt\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to UTC +00:00\n",
    "    filter_instrument = filter_instrument + timedelta(minutes=60)\n",
    "    # IMPORTANT!! : Re-Correction of datetime back to 2023\n",
    "    filter_instrument = filter_instrument + timedelta(days=365)\n",
    "    filter_instrument = filter_instrument.replace(tzinfo=timezone.utc)\n",
    "    filter_instrument = pd.Timestamp(filter_instrument)\n",
    "\n",
    "    combined_df = combined_df[combined_df['DeliveryStart']==filter_instrument]\n",
    "\n",
    "    # Important!! : Correcting the timestamp bringing it back to 2023\n",
    "    combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(days=365)\n",
    "    # Important!! : Correcting the timestamp for time zone\n",
    "    combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(hours=1)\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_data = GetData(current_product,POS,POE,product_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split product and date\n",
    "prd_id , deliveryStart = current_product.split('-')\n",
    "\n",
    "#deliveryStart with POS and POE will locate the relevant files\n",
    "# prd_id will filter the relevant data from the files\n",
    "deliveryStart_dt = datetime(*[*map(lambda _d:int(_d),[*deliveryStart.split('_')])])\n",
    "\n",
    "# IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "deliveryStart_dt = deliveryStart_dt - timedelta(days=365) # 2022 and 2023 are not leap years\n",
    "\n",
    "# IMPORTANT!!: CORRECTION #1 : Reducing date by 1 year to find the file\n",
    "deliveryStart_dt = deliveryStart_dt - timedelta(minutes=60) # TZ -01:00\n",
    "\n",
    "# Gate Closure Calculations is kept at zero as the snapshots are at 5 min interval and so is the GC\n",
    "deliveryStart_dt_GC = deliveryStart_dt + timedelta(minutes=0)\n",
    "\n",
    "#calculating the POS and POE\n",
    "POS_dt = deliveryStart_dt_GC - timedelta(minutes=POS)\n",
    "POE_dt = deliveryStart_dt_GC - timedelta(minutes=POE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since snapshots are at 5 minutes interval then all the files between these two times should be ingressed\n",
    "collect_files = []\n",
    "collect_this = POS_dt\n",
    "while collect_this <= POE_dt:\n",
    "    collect_this_str = f\"snapshot_{collect_this.year}_{collect_this.month}_{collect_this.day}_{collect_this.hour}_{collect_this.minute}_{collect_this.second}.parquet\"\n",
    "    collect_files.append(collect_this_str)\n",
    "    collect_this = collect_this + timedelta(minutes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we collect the files from source\n",
    "import os\n",
    "import pandas as pd\n",
    "matching_files_dataframes = []\n",
    "for file in collect_files:\n",
    "    file_path = os.path.join(source_path, file)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    file_str = os.path.splitext(file)[0].replace('snapshot_', '')\n",
    "    file_str = datetime(*[*map(lambda _d:int(_d) ,file_str.split('_')) ])\n",
    "    df['timestamp'] = file_str\n",
    "    matching_files_dataframes.append(df)\n",
    "\n",
    "if matching_files_dataframes:\n",
    "    combined_df = pd.concat(matching_files_dataframes, ignore_index=True)\n",
    "else:\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 1\n",
    "XBID markets have more liquidity than Intraday Local markets </br>\n",
    "For this analysis we will focus only on XBID markets </br>\n",
    "<i>We will also develop the algoframework for just XBID</i> however the framework is flexible enough to be extended for local markets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple products and instruments are trading in this snapshot\n",
    "# we need to filter the desired product and desired instrument\n",
    "#\n",
    "all_products = {\n",
    "    'HH': 'XBID_Half_Hour',\n",
    "    'QH': 'XBID_Quarter_Hour',\n",
    "    'H' : 'XBID_Hour'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterting the product we are analysing\n",
    "filter_product = all_products[product_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['Product'] == filter_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the instrument\n",
    "# formatting filter_instrument for filter query\n",
    "from datetime import timezone\n",
    "filter_instrument = deliveryStart_dt\n",
    "# IMPORTANT!! : Re-Correction of datetime back to UTC +00:00\n",
    "filter_instrument = filter_instrument + timedelta(minutes=60)\n",
    "# IMPORTANT!! : Re-Correction of datetime back to 2023\n",
    "filter_instrument = filter_instrument + timedelta(days=365)\n",
    "filter_instrument = filter_instrument.replace(tzinfo=timezone.utc)\n",
    "filter_instrument = pd.Timestamp(filter_instrument)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['DeliveryStart']==filter_instrument]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important!! : Correcting the timestamp\n",
    "combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(days=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['timestamp'] = combined_df['timestamp'] + pd.DateOffset(hours=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
